{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Loaded 24675 displacement data points.\n",
      "Loaded 19740 stress data points.\n",
      "Successfully loaded 24675 data points from FEM_PDE.csv\n",
      "Initial E guess: 1.0000 MPa\n",
      "Initial nu guess: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/torch/__init__.py:1264: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating PDE collocation points...\n",
      "Collocation point plot saved to collocation_points.png\n",
      "Generated 17740 interior points\n",
      "Generating boundary points...\n",
      "Generated 4000 boundary points\n",
      "Using 24675 displacement points and 19740 stress points.\n",
      "Starting training for 30000 epochs...\n",
      "Epoch 100/30000, Loss: 5.02255344, PDE: 0.02565440, BC: 0.05320664, Data: 0.01647897, LR_material: 0.04999890, Time: 4.04s\n",
      "    Estimated E: 1.0149, nu: 0.4291\n",
      "Epoch 200/30000, Loss: 3.58782959, PDE: 0.02657792, BC: 0.03608976, Data: 0.01175054, LR_material: 0.04999561, Time: 3.11s\n",
      "    Estimated E: 1.2088, nu: 0.4402\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import copy\n",
    "import time \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set default tensor type to float32\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Normalization\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, x_min, x_max):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        \n",
    "    def normalize(self, x):\n",
    "        return 2.0 * (x - self.x_min) / (self.x_max - self.x_min) - 1.0\n",
    "        \n",
    "    def denormalize(self, x_norm):\n",
    "        return 0.5 * (x_norm + 1.0) * (self.x_max - self.x_min) + self.x_min\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ANN\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = nn.Linear(2, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.hidden3 = nn.Linear(128, 128)\n",
    "        self.hidden4 = nn.Linear(128, 128)  \n",
    "        self.hidden5 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 2)\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = torch.tanh(self.hidden4(x))\n",
    "        x = torch.tanh(self.hidden5(x))\n",
    "        return self.output(x)  # return [u, v]\n",
    "    # Define the problem domain using the given vertices\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Vertices\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "vertices = np.array([\n",
    "    [0, 0],\n",
    "    [0, 4],\n",
    "    [5, 4],\n",
    "    [5, 0],\n",
    "    [0, 0], # Closing the polygon\n",
    "],dtype=np.float32)\n",
    "path = Path(vertices)\n",
    "def in_domain(x, y):\n",
    "    points = np.column_stack((x.cpu().numpy(), y.cpu().numpy()))\n",
    "    return torch.tensor(path.contains_points(points), dtype=torch.bool, device=device)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# BC\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Define boundary conditions with tolerance\n",
    "def BC_bottom(x, y):\n",
    "    tol = 1e-6\n",
    "    return ((torch.abs(y - 0) < tol) & (x >= 0) & (x <= 5)).squeeze()\n",
    "\n",
    "def BC_left(x, y):\n",
    "    tol = 1e-6\n",
    "    return ((torch.abs(x - 0) < tol) & (y >= 0) & (y <= 4)).squeeze()\n",
    "\n",
    "def BC_top(x, y):\n",
    "    tol = 1e-6\n",
    "    return ((torch.abs(y - 4) < tol) & (x >= 0) & (x <= 5)).squeeze()\n",
    "\n",
    "def BC_right(x, y):\n",
    "    tol = 1e-6\n",
    "    return ((torch.abs(x - 5) < tol) & (y >= 0) & (y <= 4)).squeeze()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# BC Loss (Modified for Inverse Problem)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def BC(xy, net, E, nu): # E and nu are now passed as arguments\n",
    "    x, y = xy[:, 0].unsqueeze(1), xy[:, 1].unsqueeze(1)\n",
    "    \n",
    "    output = net(xy)\n",
    "    u = output[:, 0:1]  # Split output channel (u)\n",
    "    v = output[:, 1:2]  # Split output channel (v)\n",
    "    # Get boundary conditions\n",
    "    \n",
    "    bc_b = BC_bottom(x, y)\n",
    "    bc_l = BC_left(x, y)\n",
    "    bc_t = BC_top(x, y)\n",
    "    bc_r = BC_right(x, y)\n",
    "    \n",
    "    \n",
    "    loss = F.mse_loss(u[bc_b], torch.zeros_like(u[bc_b])) + F.mse_loss(v[bc_b], torch.zeros_like(v[bc_b]))  # ux = 0, uy = 0 at bottom\n",
    "    loss += F.mse_loss(u[bc_l], torch.zeros_like(u[bc_l]))  # ux = 0 at left\n",
    "    loss += F.mse_loss(u[bc_r], torch.zeros_like(u[bc_r]))  # ux = 0 at right\n",
    "\n",
    "    \n",
    "    # Process top boundary\n",
    "    xy_top = xy[bc_t].requires_grad_(True)\n",
    "    x_top = xy_top[:, 0:1]  # x-coordinates of points on the top boundary\n",
    "    \n",
    "    output_top = net(xy_top)\n",
    "    u_top = output_top[:, 0:1]\n",
    "    v_top = output_top[:, 1:2]\n",
    "    \n",
    "    u_x_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    u_y_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    v_x_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    v_y_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate stresses (Plane Strain formulation)\n",
    "    sigma_xx_top = (E / ((1 + nu) * (1 - 2 * nu))) * ((1 - nu) * u_x_top + nu * v_y_top)\n",
    "    sigma_yy_top = (E / ((1 + nu) * (1 - 2 * nu))) * (nu * u_x_top + (1 - nu) * v_y_top)\n",
    "    sigma_xy_top = E / (2 * (1 + nu)) * (u_y_top + v_x_top)\n",
    "    \n",
    "    # Create mask for points with distributed load (x from 2 to 3)\n",
    "    load_mask = (x_top >= 2) & (x_top <= 3)\n",
    "    load_mask = load_mask.squeeze()  # Remove extra dimension\n",
    "    \n",
    "    # All points on top have sigma_xy = 0\n",
    "    loss += F.mse_loss(sigma_xy_top, torch.zeros_like(sigma_xy_top))\n",
    "\n",
    "    # Add condition for the area without load on the top edge\n",
    "    no_load_mask = ~load_mask\n",
    "    if torch.any(no_load_mask):\n",
    " \n",
    "        loss += F.mse_loss(sigma_yy_top[no_load_mask], torch.zeros_like(sigma_yy_top[no_load_mask]))\n",
    "    \n",
    "    # Points with load have sigma_yy = -0.5 (negative for compression)\n",
    "    if torch.any(load_mask):\n",
    "        loss += F.mse_loss(sigma_yy_top[load_mask], -0.5 * torch.ones_like(sigma_yy_top[load_mask]))\n",
    "    \n",
    "    # Process right boundary\n",
    "    xy_right = xy[bc_r].requires_grad_(True)\n",
    "    output_right = net(xy_right)\n",
    "    u_right = output_right[:, 0:1]\n",
    "    v_right = output_right[:, 1:2]\n",
    "    \n",
    "    u_y_right = torch.autograd.grad(u_right.sum(), xy_right, create_graph=True)[0][:, 1]\n",
    "    v_x_right = torch.autograd.grad(v_right.sum(), xy_right, create_graph=True)[0][:, 0]\n",
    "    u_x_right = torch.autograd.grad(u_right.sum(), xy_right, create_graph=True)[0][:, 0]\n",
    "    v_y_right = torch.autograd.grad(v_right.sum(), xy_right, create_graph=True)[0][:, 1]\n",
    "    sigma_xy_right = E / (2 * (1 + nu)) * (u_y_right + v_x_right)\n",
    "    loss += F.mse_loss(sigma_xy_right, torch.zeros_like(sigma_xy_right)) # Sigma_xy = 0 at right\n",
    "    # Process left boundary\n",
    "    xy_left = xy[bc_l].requires_grad_(True)\n",
    "    output_left = net(xy_left)\n",
    "    u_left = output_left[:, 0:1]\n",
    "    v_left = output_left[:, 1:2]\n",
    "    \n",
    "    u_y_left = torch.autograd.grad(u_left.sum(), xy_left, create_graph=True)[0][:, 1]\n",
    "    v_x_left = torch.autograd.grad(v_left.sum(), xy_left, create_graph=True)[0][:, 0]\n",
    "    \n",
    "    sigma_xy_left = E / (2 * (1 + nu)) * (u_y_left + v_x_left)\n",
    "    loss += F.mse_loss(sigma_xy_left, torch.zeros_like(sigma_xy_left)) # Sigma_xy = 0 at left\n",
    "\n",
    "    return loss\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Datapoint sampling (for PDE collocation points)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def generate_point_distribution(nx=100, ny=100, load_density=10, load_influence=70):\n",
    "    # Defined boundaries of the domain\n",
    "    x_min, x_max = 0.0, 5.0\n",
    "    y_min, y_max = 0.0, 4.0\n",
    "    \n",
    "    # Create random points in the domain\n",
    "    # Add number of points to be 4 times the grid size\n",
    "    num_random_points = nx * ny * 4\n",
    "    x_random = torch.rand(num_random_points, device=device) * (x_max - x_min) + x_min\n",
    "    y_random = torch.rand(num_random_points, device=device) * (y_max - y_min) + y_min\n",
    "    \n",
    "    # print(f\"Total random points generated: {len(x_random)}\")\n",
    "    \n",
    "    load_x_start, load_x_end = 2.0, 3.0\n",
    "    load_y = 4.0\n",
    "    \n",
    "    distances = torch.zeros_like(x_random)\n",
    "    \n",
    "   \n",
    "    in_x_range = (x_random >= load_x_start) & (x_random <= load_x_end)\n",
    "    \n",
    "    distances[in_x_range] = torch.abs(y_random[in_x_range] - load_y)\n",
    "    \n",
    "     \n",
    "    left_of_range = x_random < load_x_start\n",
    "    \n",
    "    distances[left_of_range] = torch.sqrt(\n",
    "        (x_random[left_of_range] - load_x_start)**2 + \n",
    "        (y_random[left_of_range] - load_y)**2\n",
    "    )\n",
    "    \n",
    "    \n",
    "    right_of_range = x_random > load_x_end\n",
    "    \n",
    "    distances[right_of_range] = torch.sqrt(\n",
    "        (x_random[right_of_range] - load_x_end)**2 + \n",
    "        (y_random[right_of_range] - load_y)**2\n",
    "    )\n",
    "    \n",
    "    base_prob = 0.2 \n",
    "    additional_prob = load_influence / (1 + (distances * load_density)**2)\n",
    "    \n",
    "    \n",
    "    keep_prob = torch.clamp(base_prob + additional_prob, 0, 1)\n",
    "    \n",
    "    \n",
    "    keep_mask = torch.rand(num_random_points, device=device) < keep_prob\n",
    "    \n",
    "    \n",
    "    x_final = x_random[keep_mask]\n",
    "    y_final = y_random[keep_mask]\n",
    "    \n",
    "    # print(f\"Final points after density-based filtering: {len(x_final)}\")\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    \n",
    "    plt.scatter(x_final.cpu().numpy(), y_final.cpu().numpy(), s=1, color='blue', alpha=0.7)\n",
    "    \n",
    "    \n",
    "    plt.plot([load_x_start, load_x_end], [load_y, load_y], 'r-', linewidth=2, label='Load line')\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Point Distribution with Enhanced Load Area Density - {len(x_final)} points')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('collocation_points.png')\n",
    "    print(\"Collocation point plot saved to collocation_points.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return x_final, y_final\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# PDE (Modified for Inverse Problem)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def PDE(x, y, net, E, nu): # E and nu are now passed as arguments\n",
    "    # Reshape 1D tensors into column vectors for concatenation\n",
    "    x_reshaped = x.unsqueeze(1) if x.dim() == 1 else x\n",
    "    y_reshaped = y.unsqueeze(1) if y.dim() == 1 else y\n",
    "    \n",
    "    xy = torch.cat([x_reshaped, y_reshaped], dim=1)\n",
    "    xy.requires_grad = True\n",
    "    \n",
    "    output = net(xy)\n",
    "    u = output[:, 0:1]\n",
    "    v = output[:, 1:2]\n",
    "    \n",
    "    u_x = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    u_y = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    v_x = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    v_y = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    \n",
    "    gamma = 0 # Distributed load (Mn/m^2)\n",
    "    \n",
    "    \n",
    "    C1 = E / ((1 + nu) * (1 - 2 * nu))\n",
    "    sigma_xx = C1 * ((1 - nu) * u_x + nu * v_y)\n",
    "    sigma_yy = C1 * (nu * u_x + (1 - nu) * v_y)\n",
    "    sigma_xy = E / (2 * (1 + nu)) * (u_y + v_x)\n",
    "    \n",
    "    f_x = torch.zeros_like(x_reshaped)\n",
    "    f_y = -gamma * torch.ones_like(y_reshaped)\n",
    "    \n",
    "    R_x = torch.autograd.grad(sigma_xx.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_x\n",
    "    R_y = torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_yy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_y\n",
    "\n",
    "    loss_x = F.mse_loss(R_x, torch.zeros_like(R_x))\n",
    "    loss_y = F.mse_loss(R_y, torch.zeros_like(R_y))\n",
    "    \n",
    "    return loss_x, loss_y\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Data Loading and Loss Functions\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def load_data(filename, device):\n",
    "    \"\"\"Loads data from FEM_PDE.csv, handling separate coordinates for displacement and stress.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(filename)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file '{filename}' not found.\")\n",
    "        print(\"Please make sure 'FEM_PDE.csv' is in the same directory as the script.\")\n",
    "        return None\n",
    "\n",
    "    # --- 1. Process Displacement Data (X, Y) ---\n",
    "    disp_cols = ['X', 'Y', 'ux', 'uy']\n",
    "    if not all(col in data.columns for col in disp_cols):\n",
    "        print(f\"Error: Missing displacement columns. Need: {', '.join(disp_cols)}\")\n",
    "        return None\n",
    "    \n",
    "    # Drop NaNs *only* from displacement data\n",
    "    disp_data = data[disp_cols].dropna()\n",
    "    if len(disp_data) == 0:\n",
    "        print(\"Error: No valid displacement data found.\")\n",
    "        return None\n",
    "\n",
    "    X_disp = torch.tensor(disp_data['X'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    Y_disp = torch.tensor(disp_data['Y'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    xy_disp_data = torch.cat([X_disp, Y_disp], dim=1)\n",
    "    u_data = torch.tensor(disp_data['ux'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    v_data = torch.tensor(disp_data['uy'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    print(f\"Loaded {len(xy_disp_data)} displacement data points.\")\n",
    "\n",
    "    # --- 2. Process Stress Data (X1, Y1) ---\n",
    "    stress_cols = ['X1', 'Y1', 'sigma_xx', 'sigma_yy', 'sigma_xy']\n",
    "    if not all(col in data.columns for col in stress_cols):\n",
    "        print(f\"Error: Missing stress columns. Need: {', '.join(stress_cols)}\")\n",
    "        return None\n",
    "        \n",
    "    # Drop NaNs *only* from stress data\n",
    "    stress_data = data[stress_cols].dropna()\n",
    "    if len(stress_data) == 0:\n",
    "        print(\"Error: No valid stress data found.\")\n",
    "        return None\n",
    "\n",
    "    X_stress = torch.tensor(stress_data['X1'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    Y_stress = torch.tensor(stress_data['Y1'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    xy_stress_data = torch.cat([X_stress, Y_stress], dim=1)\n",
    "    \n",
    "    # Convert stresses from kPa (data) to MPa (model unit)\n",
    "    s_xx_data = torch.tensor(stress_data['sigma_xx'].values / 1000.0, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    s_yy_data = torch.tensor(stress_data['sigma_yy'].values / 1000.0, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    s_xy_data = torch.tensor(stress_data['sigma_xy'].values / 1000.0, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    print(f\"Loaded {len(xy_stress_data)} stress data points.\")\n",
    "\n",
    "    # --- 3. Return all 7 tensors ---\n",
    "    return xy_disp_data, u_data, v_data, xy_stress_data, s_xx_data, s_yy_data, s_xy_data\n",
    "\n",
    "\n",
    "def data_loss(net, xy_disp_data, u_data, v_data, xy_stress_data, s_xx_data, s_yy_data, s_xy_data, E, nu):\n",
    "    \"\"\"Calculates the MSE loss from separate displacement and stress datasets.\"\"\"\n",
    "    \n",
    "    # --- 1. Displacement Loss (at X, Y coordinates) ---\n",
    "    output_disp = net(xy_disp_data)\n",
    "    u_pred = output_disp[:, 0:1]\n",
    "    v_pred = output_disp[:, 1:2]\n",
    "    \n",
    "    loss_u = F.mse_loss(u_pred, u_data)\n",
    "    loss_v = F.mse_loss(v_pred, v_data)\n",
    "\n",
    "    # --- 2. Stress Loss (at X1, Y1 coordinates) ---\n",
    "    # We must enable gradients for this pass\n",
    "    xy_stress_data.requires_grad_(True)\n",
    "    output_stress = net(xy_stress_data)\n",
    "    u_stress = output_stress[:, 0:1]\n",
    "    v_stress = output_stress[:, 1:2]\n",
    "\n",
    "    # Calculate predicted stresses using autodiff\n",
    "    u_x = torch.autograd.grad(u_stress.sum(), xy_stress_data, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    u_y = torch.autograd.grad(u_stress.sum(), xy_stress_data, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    v_x = torch.autograd.grad(v_stress.sum(), xy_stress_data, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    v_y = torch.autograd.grad(v_stress.sum(), xy_stress_data, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "\n",
    "    # Use the consistent plane strain formula\n",
    "    C1 = E / ((1 + nu) * (1 - 2 * nu))\n",
    "    s_xx_pred = C1 * ((1 - nu) * u_x + nu * v_y)\n",
    "    s_yy_pred = C1 * (nu * u_x + (1 - nu) * v_y)\n",
    "    s_xy_pred = E / (2 * (1 + nu)) * (u_y + v_x)\n",
    "\n",
    "    # Calculate MSE loss for all stress components\n",
    "    loss_s_xx = F.mse_loss(s_xx_pred, s_xx_data)\n",
    "    loss_s_yy = F.mse_loss(s_yy_pred, s_yy_data)\n",
    "    loss_s_xy = F.mse_loss(s_xy_pred, s_xy_data)\n",
    "    \n",
    "    # --- 3. Total Data Loss ---\n",
    "    total_data_loss = loss_u + loss_v + loss_s_xx + loss_s_yy + loss_s_xy\n",
    "    \n",
    "    return total_data_loss\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Training (Modified for Inverse Problem)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def train(net, optimizer_net, optimizer_material, E_param, nu_param, data_tensors, n_epochs, data_loss_weight=1.0, nx=100, ny=100):\n",
    "    scheduler_net = CosineAnnealingLR(optimizer_net, T_max=n_epochs, eta_min=1e-4)\n",
    "    scheduler_material = CosineAnnealingLR(optimizer_material, T_max=n_epochs, eta_min=1e-2)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 3000\n",
    "    \n",
    "    prev_net = None\n",
    "    \n",
    "    # Add lists for loss history\n",
    "    loss_history = []\n",
    "    pde_x_history = []\n",
    "    pde_y_history = []\n",
    "    bc_history = []\n",
    "    data_history = []\n",
    "    epoch_history = []\n",
    "    E_history = []  # NEW: Track E values\n",
    "    nu_history = []  # NEW: Track nu values\n",
    "    \n",
    "    # Create training points (collocation points) only once\n",
    "    print(\"Generating PDE collocation points...\")\n",
    "    x_inner, y_inner = generate_point_distribution(nx=nx, ny=ny)\n",
    "    print(f\"Generated {len(x_inner)} interior points\")\n",
    "    \n",
    "    # Create boundary points only once\n",
    "    print(\"Generating boundary points...\")\n",
    "    num_boundary_points = 1000\n",
    "    \n",
    "    # Create points on the bottom edge (y=0)\n",
    "    x_bottom = torch.linspace(0, 5, num_boundary_points, device=device)\n",
    "    y_bottom = torch.zeros(num_boundary_points, device=device)\n",
    "    \n",
    "    # Create points on the left edge (x=0)\n",
    "    x_left = torch.zeros(num_boundary_points, device=device)\n",
    "    y_left = torch.linspace(0, 4, num_boundary_points, device=device)\n",
    "    \n",
    "    # Create points on the top edge (y=4)\n",
    "    x_top = torch.linspace(0, 5, num_boundary_points, device=device)\n",
    "    y_top = torch.ones(num_boundary_points, device=device) * 4\n",
    "    \n",
    "    # Create points on the right edge (x=5)\n",
    "    x_right = torch.ones(num_boundary_points, device=device) * 5\n",
    "    y_right = torch.linspace(0, 4, num_boundary_points, device=device)\n",
    "    \n",
    "    # Total boundary points\n",
    "    x_b = torch.cat([x_bottom, x_left, x_top, x_right])\n",
    "    y_b = torch.cat([y_bottom, y_left, y_top, y_right])\n",
    "    \n",
    "    # Create tensor for BC function\n",
    "    xy_b = torch.stack([x_b, y_b], dim=1)\n",
    "    print(f\"Generated {len(xy_b)} boundary points\")\n",
    "    \n",
    "    # Unpack data tensors\n",
    "    xy_disp_data, u_data, v_data, xy_stress_data, s_xx_data, s_yy_data, s_xy_data = data_tensors\n",
    "    print(f\"Using {len(xy_disp_data)} displacement points and {len(xy_stress_data)} stress points.\")\n",
    "    \n",
    "    print(f\"Starting training for {n_epochs} epochs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer_net.zero_grad()\n",
    "        optimizer_material.zero_grad()\n",
    "        \n",
    "        # Constrain E and nu to physical values\n",
    "        E = F.softplus(E_param) + 1.0\n",
    "        nu = 0.01 + 0.48 * torch.sigmoid(nu_param)\n",
    "        \n",
    "        # 1. PDE Loss (Physics)\n",
    "        loss_pde_x, loss_pde_y = PDE(x_inner, y_inner, net, E, nu)\n",
    "        loss_pde = loss_pde_x + loss_pde_y\n",
    "        \n",
    "        # 2. BC Loss (Boundaries)\n",
    "        loss_bc = BC(xy_b, net, E, nu)\n",
    "        \n",
    "        # 3. Data Loss (Observations)\n",
    "        loss_data = data_loss(net, \n",
    "                              xy_disp_data, u_data, v_data, \n",
    "                              xy_stress_data, s_xx_data, s_yy_data, s_xy_data, \n",
    "                              E, nu)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        loss = loss_pde + loss_bc + data_loss_weight * loss_data\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
    "        torch.nn.utils.clip_grad_norm_([E_param, nu_param], max_norm=0.1)\n",
    "    \n",
    "        optimizer_net.step()\n",
    "        optimizer_material.step()\n",
    "        scheduler_net.step()\n",
    "        scheduler_material.step()\n",
    "        \n",
    "        # Save history (MODIFIED: Now includes E and nu)\n",
    "        loss_history.append(loss.item())\n",
    "        pde_x_history.append(loss_pde_x.item())\n",
    "        pde_y_history.append(loss_pde_y.item())\n",
    "        bc_history.append(loss_bc.item())\n",
    "        data_history.append(loss_data.item())\n",
    "        epoch_history.append(epoch + 1)  # Use epoch+1 for 1-based indexing\n",
    "        E_history.append(E.item())  # NEW\n",
    "        nu_history.append(nu.item())  # NEW\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience = 0\n",
    "            \n",
    "            prev_net = copy.deepcopy(net)\n",
    "            \n",
    "            torch.save({\n",
    "            'net_state_dict': net.state_dict(),\n",
    "            'optimizer_net_state_dict': optimizer_net.state_dict(),\n",
    "            'optimizer_material_state_dict': optimizer_material.state_dict(),\n",
    "            'scheduler_net_state_dict': scheduler_net.state_dict(),\n",
    "            'scheduler_material_state_dict': scheduler_material.state_dict(),\n",
    "            'E_param': E_param.item(),\n",
    "            'nu_param': nu_param.item(),\n",
    "            'E_final': E.item(),\n",
    "            'nu_final': nu.item(),\n",
    "            'loss': loss,\n",
    "            'epoch': epoch,\n",
    "        }, 'best_model_inverse.pth')\n",
    "                \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.8f}, '\n",
    "                  f'PDE: {loss_pde.item():.8f}, BC: {loss_bc.item():.8f}, Data: {loss_data.item():.8f}, '\n",
    "                  f'LR_material: {scheduler_material.get_last_lr()[0]:.8f}, Time: {elapsed_time:.2f}s')\n",
    "            print(f'    Estimated E: {E.item():.4f}, nu: {nu.item():.4f}')\n",
    "            start_time = time.time()\n",
    "    \n",
    "    # NEW: Save training history to CSV\n",
    "    training_df = pd.DataFrame({\n",
    "        'Epoch': epoch_history,\n",
    "        'Total_Loss': loss_history,\n",
    "        'PDE_x_Loss': pde_x_history,\n",
    "        'PDE_y_Loss': pde_y_history,\n",
    "        'BC_Loss': bc_history,\n",
    "        'Data_Loss': data_history,\n",
    "        'E_MPa': E_history,\n",
    "        'nu': nu_history,\n",
    "        'Learning_Rate_Net': [scheduler_net.get_last_lr()[0]] * len(epoch_history),\n",
    "        'Learning_Rate_Material': [scheduler_material.get_last_lr()[0]] * len(epoch_history) \n",
    "    })\n",
    "    training_df.to_csv('training_history.csv', index=False)\n",
    "    print(\"Training history saved to training_history.csv\")\n",
    "    \n",
    "    # Create graph Loss history\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot total loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epoch_history, loss_history, 'b-', label='Total Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Training Loss History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot component losses\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epoch_history, pde_x_history, 'r-', label='PDE_x Loss')\n",
    "    plt.plot(epoch_history, pde_y_history, 'g-', label='PDE_y Loss')\n",
    "    plt.plot(epoch_history, bc_history, 'b-', label='BC Loss')\n",
    "    plt.plot(epoch_history, data_history, 'm-', label=f'Data Loss (x{data_loss_weight})')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Component Losses (log scale)')\n",
    "    plt.title('Component Losses History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_history_inverse.png')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "# --- Plot E evolution ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_history, E_history, 'b-', linewidth=1.5, label='Predicted E')\n",
    "    plt.axhline(y=15, color='k', linestyle='--', linewidth=1.2, label='True E = 15 MPa')  # เส้นประ\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(\"Young's Modulus E (MPa)\")\n",
    "    plt.title('Evolution of E during Training')\n",
    "    plt.grid(False)\n",
    "    plt.legend()\n",
    "\n",
    "# --- Plot ν evolution ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_history, nu_history, 'r-', linewidth=1.5, label='Predicted ν')\n",
    "    plt.axhline(y=0.3, color='k', linestyle='--', linewidth=1.2, label='True ν = 0.3')  # เส้นประ\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(\"Poisson's Ratio ν\")\n",
    "    plt.title('Evolution of ν during Training')\n",
    "    plt.grid(False)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('material_parameters_evolution.png')\n",
    "    plt.close()\n",
    "    print(\"Material parameters evolution plot saved to material_parameters_evolution.png\")\n",
    "\n",
    "    \n",
    "    # Additional return value for metrics for tracking training progress\n",
    "    metrics = {\n",
    "        'loss_history': loss_history,\n",
    "        'pde_x_history': pde_x_history,\n",
    "        'pde_y_history': pde_y_history,\n",
    "        'bc_history': bc_history,\n",
    "        'data_history': data_history,\n",
    "        'epoch_history': epoch_history,\n",
    "        'E_history': E_history,  \n",
    "        'nu_history': nu_history  \n",
    "    }\n",
    "    \n",
    "    # Load the best model state\n",
    "    print(\"Loading best model from 'best_model_inverse.pth'\")\n",
    "    checkpoint = torch.load('best_model_inverse.pth')\n",
    "    net.load_state_dict(checkpoint['net_state_dict'])\n",
    "    final_E = checkpoint['E_final']\n",
    "    final_nu = checkpoint['nu_final']\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return net, metrics, final_E, final_nu\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Plot result (Modified for Inverse Problem)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def plot_results(net, E, nu, output_filename='Footing_data_inverse.csv'): # E, nu passed as args\n",
    "    nx, ny = 100, 100\n",
    "    X = torch.linspace(0, 5.0, nx, device=device)  \n",
    "    Y = torch.linspace(0, 4.0, ny, device=device)  \n",
    "    \n",
    "    xx, yy = torch.meshgrid(X, Y, indexing='ij')\n",
    "    \n",
    "    X = xx.flatten()\n",
    "    Y = yy.flatten()\n",
    "    \n",
    "   \n",
    "    mask = in_domain(X, Y)\n",
    "    mask_np = mask.cpu().numpy()  \n",
    "    \n",
    "    \n",
    "    XY = torch.stack([X, Y], dim=1)\n",
    "    XY.requires_grad_(True)\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        output = net(XY)\n",
    "        U = output[:, 0:1]  \n",
    "        V = output[:, 1:2]  \n",
    "        \n",
    "        U_x = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        U_y = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        V_x = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        V_y = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        \n",
    "        \n",
    "        # *** CORRECTION: Using Plane Strain equations ***\n",
    "        C1 = E / ((1 + nu) * (1 - 2 * nu))\n",
    "        sigma_xx = C1 * ((1 - nu) * U_x + nu * V_y)\n",
    "        sigma_yy = C1 * (nu * U_x + (1 - nu) * V_y)\n",
    "        sigma_xy = E / (2 * (1 + nu)) * (U_y + V_x)\n",
    "    \n",
    "    \n",
    "    X_np = X.detach().cpu().numpy()\n",
    "    Y_np = Y.detach().cpu().numpy()\n",
    "    U_full = U.detach().cpu().numpy().squeeze()\n",
    "    V_full = V.detach().cpu().numpy().squeeze()\n",
    "    sigma_xx_full = sigma_xx.detach().cpu().numpy().squeeze()*1000 # Convert MPa to kPa for plotting\n",
    "    sigma_yy_full = sigma_yy.detach().cpu().numpy().squeeze()*1000 # Convert MPa to kPa for plotting\n",
    "    sigma_xy_full = sigma_xy.detach().cpu().numpy().squeeze()*1000 # Convert MPa to kPa for plotting\n",
    "    \n",
    "    \n",
    "    magnitude = np.sqrt(U_full**2 + V_full**2)\n",
    "    \n",
    "    \n",
    "    df_out = pd.DataFrame({\n",
    "        'X': X_np[mask_np],\n",
    "        'Y': Y_np[mask_np],\n",
    "        'ux': U_full[mask_np],\n",
    "        'uy': V_full[mask_np],\n",
    "        'sigma_xx_kPa': sigma_xx_full[mask_np],\n",
    "        'sigma_yy_kPa': sigma_yy_full[mask_np],\n",
    "        'sigma_xy_kPa': sigma_xy_full[mask_np],\n",
    "        'magnitude': magnitude[mask_np]\n",
    "    })\n",
    "    \n",
    "    \n",
    "    df_out.to_csv(output_filename, index=False)\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "    \n",
    "    \n",
    "    mask_cpu = mask.cpu().numpy()\n",
    "    U_masked = np.ma.masked_array(U_full, mask=~mask_cpu)\n",
    "    V_masked = np.ma.masked_array(V_full, mask=~mask_cpu)\n",
    "    sigma_xx_masked = np.ma.masked_array(sigma_xx_full, mask=~mask_cpu)\n",
    "    sigma_yy_masked = np.ma.masked_array(sigma_yy_full, mask=~mask_cpu)\n",
    "    sigma_xy_masked = np.ma.masked_array(sigma_xy_full, mask=~mask_cpu)\n",
    "    magnitude_masked = np.ma.masked_array(magnitude, mask=~mask_cpu)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    fig_title = f\"Inverse Problem Results (Estimated E={E:.2f} MPa, $\\\\nu$={nu:.3f})\"\n",
    "    plt.suptitle(fig_title, fontsize=16)\n",
    "    \n",
    "    # Displacement plots\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), U_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Displacement ux (m)')\n",
    "    plt.title('Displacement in x-direction')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), V_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Displacement uy (m)')\n",
    "    plt.title('Displacement in y-direction')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), magnitude_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Displacement magnitude (m)')\n",
    "    plt.title('Displacement magnitude')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Stress plots\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), sigma_xx_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Stress $\\\\sigma_{xx}$ (kPa)')\n",
    "    plt.title('Normal stress $\\\\sigma_{xx}$')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "   \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), sigma_yy_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Stress $\\\\sigma_{yy}$ (kPa)')\n",
    "    plt.title('Normal stress $\\\\sigma_{yy}$')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), sigma_xy_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Stress $\\\\sigma_{xy}$ (kPa)')\n",
    "    plt.title('Shear stress $\\\\sigma_{xy}$')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust for suptitle\n",
    "    plt.savefig('results_plots_inverse.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Result plots saved to results_plots_inverse.png\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Main execution (Modified for Inverse Problem)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # --- 1. Load Data ---\n",
    "    data_filename = 'FEM_PDE.csv'\n",
    "    data_tensors = load_data(data_filename, device)\n",
    "    \n",
    "    if data_tensors is not None:\n",
    "        print(f\"Successfully loaded {len(data_tensors[0])} data points from {data_filename}\")\n",
    "        \n",
    "        # --- 2. Initialize network ---\n",
    "        net = Net().to(device)\n",
    "        \n",
    "        # --- 3. Initialize Trainable Material Parameters ---\n",
    "        # We initialize them as unconstrained parameters.\n",
    "        # The 'train' function will map them to valid ranges.\n",
    "        \n",
    "        # Initial guess for E: 15.0. \n",
    "        # We need to find 'x' such that softplus(x) + 1.0 = 15.0\n",
    "        # softplus(x) = 14.0 -> log(1 + exp(x)) = 14.0 -> x approx 14.0\n",
    "        initial_E_param = torch.tensor([-10.0], device=device, requires_grad=True)\n",
    "        \n",
    "        # Initial guess for nu: 0.3. \n",
    "        # We need to find 'x' such that 0.01 + 0.48 * sigmoid(x) = 0.3\n",
    "        # sigmoid(x) = (0.3 - 0.01) / 0.48 = 0.604\n",
    "        # x = log(0.604 / (1 - 0.604)) approx 0.42\n",
    "        initial_nu_param = torch.tensor([-1.466], device=device, requires_grad=True)\n",
    "\n",
    "        print(f\"Initial E guess: {(F.softplus(initial_E_param) + 1.0).item():.4f} MPa\")\n",
    "        print(f\"Initial nu guess: {(0.01 + 0.48 * torch.sigmoid(initial_nu_param)).item():.4f}\")\n",
    "\n",
    "        # --- 4. Optimizer ---\n",
    "        # Add E and nu parameters to the optimizer\n",
    "        optimizer_net = optim.Adam(net.parameters(), lr=0.001)\n",
    "        optimizer_material = optim.Adam([initial_E_param, initial_nu_param], lr=0.05)\n",
    "        \n",
    "        # --- 5. Train the network ---\n",
    "        # Set a data loss weight. This is a critical hyperparameter to tune.\n",
    "        # If data loss is too low, increase weight. If PDE/BC losses are ignored, decrease it.\n",
    "        data_weight = 300.0 \n",
    "        \n",
    "        trained_net, metrics, final_E, final_nu = train(\n",
    "            net=net, \n",
    "            optimizer_net=optimizer_net,\n",
    "            optimizer_material=optimizer_material,\n",
    "            E_param=initial_E_param, \n",
    "            nu_param=initial_nu_param,\n",
    "            data_tensors=data_tensors,\n",
    "            n_epochs=30000, # You may need more epochs (e.g., 20k-50k)\n",
    "            data_loss_weight=data_weight,\n",
    "            nx=100, # Collocation points grid density\n",
    "            ny=100\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Inverse Analysis Complete ---\")\n",
    "        print(f\"Estimated Young's Modulus (E): {final_E:.4f} MPa\")\n",
    "        print(f\"Estimated Poisson's Ratio (nu): {final_nu:.4f}\")\n",
    "        \n",
    "        # --- 6. Plot results ---\n",
    "        try:\n",
    "            plot_results(trained_net, final_E, final_nu)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in plot_results: {str(e)}\")\n",
    "    else:\n",
    "        print(\"Exiting due to data loading error.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
